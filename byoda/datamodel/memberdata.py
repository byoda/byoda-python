'''
Class for modeling an element of data of a member

This class has several static methods that are called from the
code generated from the Jinja2 templates to process the
data as requested in the call to the REST Data API.

Processing a REST Data API call involves the following steps:
1. the schema/version/action specific function generated by the
'pydantic-model-rest-apis.py.jinja2 template
2. The MemberData.<action> method, which logs the request, performs parameter
validation and gathers all the data needed to process the request
3. The MemberData.<action>_data method, which does the actual
work

:maintainer : Steven Hessing <steven@byoda.org>
:copyright  : Copyright 2021, 2022, 2023, 2024
:license    : GPLv3
'''

from uuid import UUID
from typing import TypeVar
from logging import Logger, getLogger
from datetime import datetime
from datetime import timezone
from datetime import timedelta
from itertools import combinations as iter_combinations

import orjson

from pydantic import Base64Str

from websockets.legacy.client import WebSocketClientProtocol
from websockets.exceptions import ConnectionClosedOK
from websockets.exceptions import ConnectionClosedError

from uvicorn.protocols.utils import ClientDisconnected

from opentelemetry.trace import get_tracer
from opentelemetry.sdk.trace import Tracer

from byoda.datamodel.dataclass import SchemaDataArray
from byoda.datamodel.dataclass import SchemaDataObject
from byoda.datamodel.datafilter import DataFilterSet

from byoda.datamodel.table import Table
from byoda.datamodel.table import ResultData
from byoda.datamodel.table import QueryResult

from byoda.datamodel.data_proxy import DataProxy


from byoda.datamodel.pubsub_message import PubSubDataAppendMessage
from byoda.datamodel.pubsub_message import PubSubDataMutateMessage
from byoda.datamodel.pubsub_message import PubSubDataDeleteMessage
from byoda.datamodel.pubsub_message import PubSubDataMessage

from byoda.datatypes import IdType
from byoda.datatypes import DataType
from byoda.datatypes import DataRequestType
from byoda.datatypes import NetworkLink
from byoda.datatypes import DataFilterType
from byoda.datatypes import MARKER_NETWORK_LINKS
from byoda.datatypes import MARKER_DATA_LOGS

from byoda.models.data_api_models import QueryModel
from byoda.models.data_api_models import AppendModel

from byoda.datacache.counter_cache import CounterCache

from byoda.requestauth.requestauth import RequestAuth

from byoda.secrets.data_secret import InvalidSignature

from byoda.storage import FileMode

from byoda.storage.pubsub import PubSub



from byoda.util.paths import Paths

from byoda import config

from byoda.exceptions import ByodaValueError

# These imports are only used for typing
from .schema import Schema
from .dataclass import SchemaDataItem

Account = TypeVar('Account')
Member = TypeVar('Member')
EdgeResponse = TypeVar('EdgeResponse')
CacheStore = TypeVar('CacheStore')
DataStore = TypeVar('DataStore')
PodServer = TypeVar('PodServer')

_LOGGER: Logger = getLogger(__name__)
TRACER: Tracer = get_tracer(__name__)

MAX_FILE_SIZE = 65536

RECURSIVE_QUERY_TTL = 300
QUERY_EXPIRATION = timedelta(seconds=RECURSIVE_QUERY_TTL)


class MemberData(dict):
    '''
    Generic data object for the storing data as defined
    by the schema of services
    '''

    __slots__: list[str] = [
        'member', 'unvalidated_data', 'paths'
    ]

    def __init__(self, member: Member) -> None:
        self.member: Member = member
        self.unvalidated_data: dict = None

        self.paths: Paths = member.paths

    async def initialize(self) -> None:
        '''
        Initializes the data for a new membership. Every service
        contract must include The 'member_id', 'joined', 'schema_versions',
        and 'auto_upgrade' fields.

        :returns: (none)
        :raises ValueError: if there is no data in the data store for the
        'member' data class
        '''

        member: Member = self.member
        schema: Schema = member.schema

        member.joined = datetime.now(timezone.utc)
        member.schema_versions = [schema.version]

        member_data: dict[str, str | int | bool] = {
            'member_id': str(member.member_id),
            'joined': member.joined.isoformat(),
            'schema_versions': member.schema_versions,
            'auto_upgrade': member.auto_upgrade,
        }

        data_store: DataStore = config.server.data_store
        await data_store.mutate(
            member.member_id, 'member', member_data, None, None, None
        )

    async def load_member_settings(self) -> None:
        data_store: DataStore = config.server.data_store
        member: Member = self.member
        data_class: SchemaDataObject = member.schema.data_classes['member']
        data: list[QueryResult] = await data_store.query(
            member.member_id, data_class, None, 1, None, [],
        )

        if not data:
            raise ValueError('No member data found')

        data_class: SchemaDataObject = member.schema.data_classes['member']
        member_data: dict[str, any] = data_class.normalize(data[0][0])
        return member_data

    def normalize(self) -> None:
        '''
        Updates data values to match data type as defined in JSON-Schema,
        ie. for UUIDs and datetime
        '''

        schema: Schema = self.member.service.schema

        if not schema:
            raise ValueError('Schema has not yet been loaded')

        data_classes: dict[str, SchemaDataItem] = schema.data_classes
        for field, value in self.items():
            if field not in data_classes:
                raise ValueError(
                    f'Found data field {field} not in the data classes '
                    'for the schema'
                )

            normalized = data_classes[field].normalize(value)
            self[field] = normalized

    def validate(self):
        '''
        Validates the unvalidated data against the schema
        '''

        try:
            if self.unvalidated_data:
                _LOGGER.debug(
                    f'Validating {len(self.unvalidated_data)} bytes of data'
                )
                self.member.schema.validator.is_valid(self.unvalidated_data)
            else:
                _LOGGER.debug('No unvalidated data to validate')
        except Exception as exc:
            _LOGGER.warning(
                'Failed to validate data for service_id '
                f'{self.member.service_id}: {exc}'
            )
            raise

    async def load_protected_shared_key(self) -> None:
        '''
        Reads the protected symmetric key from file storage. Support
        for changing symmetric keys is currently not supported.
        '''

        filepath = self.paths.get(
            self.paths.MEMBER_DATA_SHARED_SECRET_FILE
        )

        try:
            protected = await self.member.storage_driver.read(
                filepath, file_mode=FileMode.BINARY
            )
            self.member.data_secret.load_shared_key(protected)
        except OSError:
            _LOGGER.error(
                'Can not read the protected shared key for service %s from %s',
                self.member.service_id, filepath
            )
            raise

    async def save_protected_shared_key(self) -> None:
        '''
        Saves the protected symmetric key
        '''

        filepath: str = self.paths.get(
            self.paths.MEMBER_DATA_SHARED_SECRET_FILE
        )

        await self.member.storage_driver.write(
            filepath, self.member.data_secret.protected_shared_key,
            file_mode=FileMode.BINARY
        )

    async def load_network_links(self, relations: str | list[str] | None = None
                                 ) -> list[NetworkLink]:
        '''
        Loads the network links for the membership. Used by the access
        control logic.
        '''

        filter_set: DataFilterSet | None = None

        if relations:
            # DataFilter logic uses 'and' logic when multiple filters are
            # specified. So we only use DataFilterSet when we have a single
            # relation to filter on.
            relation: str | list[str] = relations
            if isinstance(relations, list) and len(relations) == 1:
                relation = relations[0]

            if isinstance(relation, str):
                link_filter: dict[str, dict[str, str]] = {
                    'relation': {
                        'eq': relation
                    }
                }
                filter_set = DataFilterSet(link_filter)

        data_store: DataStore = config.server.data_store
        member: Member = self.member
        schema: Schema = member.schema
        data_class: SchemaDataArray = schema.data_classes[MARKER_NETWORK_LINKS]

        query_data: list[QueryResult] = await data_store.query(
            member.member_id, data_class, filters=filter_set
        ) or []

        # If more than 1 relation was provided, then we filter here
        # ourselves instead of using DataFilterSet
        data: list[NetworkLink] = []
        for network_link, _ in query_data:
            if (relations and isinstance(relations, list)
                    and len(relations) > 1):
                if network_link['relation'] in relations:
                    data.append(NetworkLink(**network_link))
                else:
                    # relation of link is not in the filter of
                    # wanted relations
                    continue
            else:
                data.append(NetworkLink(**network_link))

        return data

    @TRACER.start_as_current_span('MemberData.add_log_entry')
    async def add_log_entry(self, remote_addr: str, auth: RequestAuth,
                            operation: str | DataRequestType, source: str,
                            class_name: str,
                            filters: DataFilterSet | None = None,
                            depth: int | None = None,
                            relations: list[str] = [],
                            remote_member_id: UUID | None = None,
                            query_id: UUID | None = None,
                            origin_member_id: UUID | None = None,
                            query_timestamp: datetime | None = None,
                            origin_signature: str | None = None,
                            signature_format_version: int | None = None,

                            message: str = None) -> None:
        '''
        Adds an entry to data log

        :param remote_addr: the IP address or hostname of the host that
        originated the request
        :param auth: information about the authentication used for the request
        :param operation: the operation that was performed by the API call
        :param source: source originating the log entry
        :param class_name: the name of the class that was queried
        :param filters: the filters to apply to the data
        :param depth: the number of hops to proxy the request
        :param relations: list of relations that should be queried
        :param remote_member_id: the member id of the member that the request
        should be proxied to
        :param query_id: the query id of the query that was performed
        :param origin_member_id: the member_id of the member that originated
        the request
        :param query_timestamp: the timestamp of the query as set by the pod
        that originated the request
        :param origin_signature: the signature of the query as set by the pod
        that originated the request
        :param signature_format_version: the version of the signature format
        :returns: (none)
        '''

        if not config.log_requests:
            return

        server: PodServer = config.server
        data_store: DataStore = server.data_store

        if not isinstance(filters, DataFilterSet):
            filter_set: DataFilterSet = DataFilterSet(filters)
        else:
            filter_set: DataFilterSet = filters

        if isinstance(operation, DataRequestType):
            operation = operation.value

        data: dict[str, str | UUID | datetime | float | int] = {
            'created_timestamp': datetime.now(timezone.utc),
            'remote_addr': remote_addr,
            'remote_id': auth.id,
            'remote_id_type': auth.id_type.value.rstrip('s-'),
            'operation': operation,
            'object': class_name,
            'query_filters': str(filter_set),
            'query_depth': depth,
            'query_relations': ', '.join(relations or []),
            'query_id': query_id,
            'query_remote_member_id': remote_member_id,
            'origin_member_id': origin_member_id,
            'origin_timestamp': query_timestamp,
            'origin_signature': origin_signature,
            'signature_format_version': signature_format_version,
            'source': source,
            'message': message,
        }

        member: Member = self.member

        required_fields: list[str] = await MemberData.get_required_field_names(
            member.service_id, class_name
        )
        cursor: str = Table.get_cursor_hash(
            data, member.member_id, required_fields
        )

        schema: Schema = member.schema
        data_class: SchemaDataArray = schema.data_classes[MARKER_DATA_LOGS]
        await data_store.append(
            self.member.member_id, data_class, data, cursor,
            auth.id, auth.id_type
        )

        _LOGGER.debug(f'Appended data log entry: {orjson.dumps(data)}')

    @staticmethod
    async def get_required_field_names(service_id, class_name) -> list[str]:
        '''
        Get the list of the names of fields that are required to have a value
        '''

        server: PodServer = config.server

        account: Account = server.account
        member: Member = await account.get_membership(service_id)
        schema: Schema = member.schema
        target_class: SchemaDataArray = schema.data_classes[class_name]
        if target_class.referenced_class:
            target_class: SchemaDataObject = target_class.referenced_class

        return target_class.required_fields

    @staticmethod
    @TRACER.start_as_current_span('MemberData.get')
    async def get(service_id: int, class_name: str, query_id: UUID,
                  fields: list[str], data_filter: DataFilterType,
                  first: int, after: str | None, depth: int,
                  relations: list[str], remote_member_id: UUID,
                  timestamp: float, origin_member_id: UUID,
                  origin_signature: Base64Str, signature_format_version: int,
                  query: QueryModel, remote_addr: str, auth: RequestAuth,
                  class_ref: callable, edge_class_ref: callable,
                  log_data: dict[str, any] = {}
                  ) -> list[EdgeResponse]:
        '''
        Extracts the requested data object.

        This function is called from the Python3 code generated by the
        jsonschema-to-python converter

        :param service_id: the service being queried
        :param class_name: the class for which to change the dict
        :param query_id: the query ID of the incoming request
        :param fields: the fields requested to be returned
        :param remote_addr: host that originated the Data query
        :param data_filter: the filter to apply to the data
        :param first: how many objects to return
        :param after: cursor for the record after which data should be returned
        :param depth: number of hops to proxy the request to
        :param relations: list of network relations to proxy the request to
        :param remote_member_id: the UUID of the member to proxy the request to
        :param timestamp: the timestamp of the query as set by the pod
        originating the query, used to expire old queries
        :param origin_member_id: the ID of the member that originated the query
        :param origin_signature: the signature of the query as set by the pod
        originating the query
        :param signature_format_version: the version of the signature format
        :param query: the REST query requested by the client, provided to
        use as basis for proxying the request
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param class_ref: the Pydantic-derived data class to validate the data
        :param edge_class_ref: the data class to normalize the results to
        :param log_data: additional data to log
        :returns: list of 'edge responses', as defined in the Request Modeling
        Jinja templates for each data class
        :raises: ValueError
        '''

        _LOGGER.debug('Got REST GET Data API query', extra=log_data)

        server: PodServer = config.server

        account: Account = server.account
        member: Member = await account.get_membership(service_id)

        # We want to know who send us the query so that if we have to
        # proxy, we don't proxy the message back to the sender
        sending_member_id: UUID = auth.member_id
        log_data['sending_member_id'] = sending_member_id

        if query_id:
            _LOGGER.debug('Query received with query_id', extra=log_data)
            if not await member.query_cache.set(query_id, auth.id):
                _LOGGER.debug('Received duplicate query_id', extra=log_data)
                raise ByodaValueError(f'Duplicate query id: {query_id}')

        if origin_member_id or origin_signature:
            try:
                await DataProxy.verify_signature(
                    service_id, relations, data_filter,
                    timestamp, origin_member_id,
                    origin_signature, signature_format_version
                )
            except InvalidSignature:
                raise ByodaValueError(
                    'Failed verification of signature for recursive query '
                    f'received from {auth.id} with IP {auth.remote_addr} '
                )

            timestamp = timestamp.replace(tzinfo=timezone.utc)
            if timestamp - datetime.now(timezone.utc) > QUERY_EXPIRATION:
                _LOGGER.debug(
                    f'TTL of {RECURSIVE_QUERY_TTL} seconds expired, '
                    'not proxying this request', extra=log_data
                )
                depth = 0
        elif depth > 0:
            # If no origin_member_id has been provided then the request
            # must come from our own membership

            if not query_id:
                raise ValueError('Recursive query without query_id')

            if (auth.id_type != IdType.MEMBER or
                    auth.member_id != member.member_id):
                raise ByodaValueError(
                    'Received a recursive query without signature '
                    'submitted by someone else than our membership'
                )

        schema: Schema = member.schema
        data_class: SchemaDataItem = schema.data_classes[class_name]
        _LOGGER.debug('Collecting data', extra=log_data)

        filter_set = DataFilterSet(data_filter, data_class=data_class)

        await member.data.add_log_entry(
            remote_addr, auth, 'query', 'REST DATA API', class_name,
            filters=data_filter, depth=depth, relations=relations,
            remote_member_id=remote_member_id, query_id=query_id,
            origin_member_id=origin_member_id, query_timestamp=timestamp,
            origin_signature=origin_signature,
            signature_format_version=signature_format_version
        )

        required_fields: set[str]
        referenced_class: SchemaDataObject | None = data_class.referenced_class
        if (data_class.type == DataType.ARRAY and referenced_class
                and referenced_class.type == DataType.OBJECT):
            required_fields = referenced_class.required_fields
        elif data_class.type == DataType.OBJECT:
            required_fields = data_class.required_fields
        else:
            required_fields = None
            _LOGGER.warning('Unrecognized data structure', extra=log_data)

        if fields:
            # We intentionally do not update the query.fields before
            # proxying recursive queries as that would invalidate the
            # signature of the query
            fields |= set(required_fields)

        all_data: list[edge_class_ref] = []

        if depth:
            _LOGGER.debug('Got recursive query', extra=log_data)
            remote_data: list[dict[str, object]] = \
                await MemberData._get_data_from_pods_recursively(
                    member, class_name, query, sending_member_id, log_data
                )
            data_item: dict[str, object]
            for data_item in remote_data:
                modeled_data: dict[str, object] = class_ref.model_validate(
                    data_item['node']
                )
                edge_data: any = edge_class_ref(
                    cursor=data_item['cursor'], origin=data_item['origin'],
                    node=data_item['node']
                )

                all_data.append(edge_data)

            if remote_member_id:
                # Recursive queries specifying remote_member_id should
                # not include data from our own pod so we're done here
                _LOGGER.debug(
                    'Got items from remote member',
                    extra=log_data | {'items_retrieved': len(all_data)}
                )
                return all_data

        # We ask for 'query.first + 1) as we want to know if there are
        # more items available for pagination
        with TRACER.start_as_current_span('MemberData.get from cache store'):
            if data_class.cache_only:
                cache_store: CacheStore = server.cache_store
                data: list[QueryResult] = await cache_store.query(
                    member.member_id, data_class, filter_set,
                    first + 1, after, fields
                ) or []
                log_data['items_retrieved'] = len(data)
                _LOGGER.debug(
                    'Got items from the cache store', extra=log_data
                )
            else:
                data_store: DataStore = server.data_store
                data: list[QueryResult] = await data_store.query(
                    member.member_id, data_class, filter_set,
                    first + 1, after, fields
                ) or []
                log_data['items_retrieved'] = len(data)
                _LOGGER.debug(
                    'Got items from the data store', extra=log_data
                )

        with TRACER.start_as_current_span('MemberData.get collect'):
            # TODO: see how we can return metadata for the retrieved data
            # even if we have retrieved recursive data that does not
            # include metadata
            data_item: ResultData
            for data_item, meta_data in data:
                modeled_data: dict[str, object] = class_ref.model_validate(
                    data_item
                )
                # results for queries for 'objects' may not have a cursor
                cursor: str = meta_data.get('cursor', '')
                log_data['cursor'] = cursor
                edge_data = edge_class_ref(
                    cursor=cursor, origin=member.member_id, node=modeled_data
                )
                all_data.append(edge_data)
                _LOGGER.debug('Adding item to results', extra=log_data)

            _LOGGER.debug('Got total items from data', extra=log_data)

        return all_data

    @TRACER.start_as_current_span('MemberData._get_data_from_pods')
    @staticmethod
    async def _get_data_from_pods_recursively(
            member: Member, class_name: str, query: QueryModel,
            sending_member_id: UUID, log_data: dict[str, any]
    ) -> list[dict[str, object]]:
        '''
        Gets data from other pods, as requested by recursive query

        This method updates the provided query object

        :param member: our membership of the service
        :param class_name: the name of the class in the query
        :param query: the received query object
        :param sending_member_id: the member id of the member that sent the
        :param log_data: additional data to log
        query
        '''

        proxy = DataProxy(member)

        if not query.origin_member_id:
            # Our membership submitted the query so let's
            # add needed data and sign the request
            query.origin_member_id = member.member_id
            query.signature_format_version = 1

            query.origin_signature = proxy.create_signature(
                member.service_id, query.relations, query.filter,
                query.timestamp, query.origin_member_id, log_data=log_data
            )

        all_data: list[dict[str, any]] = await proxy.proxy_query_request(
            class_name, query, sending_member_id, log_data
        )

        _LOGGER.debug(
            'Collected items from the network',
            extra=log_data | {'items': len(all_data)}
        )

        return all_data

    @staticmethod
    async def updates(service_id: int, class_name: str,
                      query_id: UUID, relations: list[str], depth: int,
                      updates_filter: DataFilterType,
                      websocket: WebSocketClientProtocol, auth: RequestAuth,
                      ) -> EdgeResponse:
        '''
        Provides updates to the websocket if an array at the root level of the
        schema has been updated.

        This function is called from the code generated from the Jinja2
        template for pydantic models for the JSON-Schema

        :param service_id: the service being queried
        :param class_name: the class for which to change the dict
        :param query_id: the query ID of the incoming request
        :param depth: the max distance of received data to proxy to this
        websocket
        :param updates_filter: the filter to apply to the updates
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param updates_model: the request we received
        :returns: None
        '''

        remote_addr: str = websocket.client.host
        log_data: dict[str, any] = {
            'remote_addr': remote_addr,
            'data_class': class_name,
            'service_id': service_id,
            'depth': depth,
            'query_id': query_id,
            'auth_id': auth.id,
            'auth_id_type': auth.id_type,
        }
        _LOGGER.debug(
            'Received Data Updates API request', extra=log_data
        )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.UPDATES, 'REST Data',
            class_name, query_id=query_id, depth=depth, relations=relations
        )

        schema: Schema = member.schema
        data_class: SchemaDataItem = schema.data_classes[class_name]

        sub = PubSub.setup(
            data_class.name, data_class, member.schema, is_sender=False
        )

        while True:
            messages = await sub.recv(
                expected_class_name=data_class.referenced_class.name
                )
            _LOGGER.debug(
                'Received messages', extra={'messages': len(messages)}
            )

            message: PubSubDataMessage
            for message in messages or []:
                data: dict[str, object]
                # We run the data through the filters but the filters
                # work on and return arrays
                filtered_items: list[dict]
                if isinstance(message.node, int):
                    filtered_items = [message.node]
                else:
                    filtered_items: list[dict] = DataFilterSet.filter(
                        updates_filter, [message.node],
                        data_class.referenced_class
                    )
                _LOGGER.debug(
                    'Still have items after filtering',
                    extra=log_data | {'filtered_items': len(filtered_items)}
                )
                for item in filtered_items:
                    data = {
                        'node': item,
                        'cursor': message.cursor,
                        'filter': message.filter,
                        'origin_id': message.origin_id,
                        'origin_id_type': message.origin_id_type,
                        'origin_class_name': message.origin_class_name,
                        'query_id': query_id,
                        'hops': 0,
                    }

                    _LOGGER.debug('Sending update for class', extra=log_data)

                    text: str = orjson.dumps(data).decode('utf-8')
                    try:
                        await websocket.send_text(text)
                    except (ConnectionClosedOK, ConnectionClosedError,
                            ClientDisconnected) as exc:
                        _LOGGER.debug(
                            f'WebSocket connection closed: {exc}',
                            extra=log_data)
                        return

    @staticmethod
    async def counter(service_id: int, class_name: str,
                      query_id: UUID, depth: int, relations: list[str],
                      counter_filter: DataFilterType,
                      websocket: WebSocketClientProtocol,
                      auth: RequestAuth) -> int:
        '''
        Provides counters if an array at the root level of the schema
        has been updated.

        This function is called from the code generated from the Jinja2
        template for pydantic models for the JSON-Schema

        :param service_id: the service being queried
        :param class_name: the class for which to change the dict
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param updates_model: the request we received
        :returns: None
        '''

        remote_addr: str = websocket.client.host
        log_data: dict[str, any] = {
            'remote_addr': remote_addr,
            'data_class': class_name,
            'service_id': service_id,
            'depth': depth,
            'query_id': query_id,
            'auth_id': auth.id,
            'auth_id_type': auth.id_type,
        }

        _LOGGER.debug(
            'Received REST Data Updates API request', extra=log_data
        )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)
        member_id: UUID = member.member_id

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.UPDATES, 'REST Data',
            class_name, query_id=query_id, depth=depth, relations=relations
        )

        data_class: SchemaDataItem = member.schema.data_classes[class_name]
        sub: PubSub = PubSub.setup(
            data_class.name, data_class, member.schema, is_sender=False
        )

        counter_cache: CounterCache = member.counter_cache
        data_store: DataStore = server.data_store
        table: Table = data_store.get_table(member.member_id, class_name)

        current_counter_value: int = await counter_cache.get(
            class_name, counter_filter, table
        )

        # TODO: never nest!
        while True:
            messages: list[dict[str, any]] = await sub.recv()
            _LOGGER.debug(
                'Received messages',
                extra=log_data | {'messages': len(messages)}
            )

            for message in messages or []:
                matches_filter = True
                if counter_filter:
                    for field_name, value in counter_filter.items():
                        if message.node.get(field_name) != value:
                            matches_filter = False

                if not matches_filter:
                    _LOGGER.debug(
                        'Message did not match filter', extra=log_data)
                    continue

                _LOGGER.debug('Message matched filter', extra=log_data)

                counter_value: int = await counter_cache.get(
                    class_name, counter_filter
                )

                if counter_value != current_counter_value:
                    data: dict[str, str | int | UUID] = {
                        'cursor': '', 'origin': member_id,
                        'query_id': query_id, 'counter': counter_value
                    }

                    _LOGGER.debug(
                        'Sending counter update for', extra=log_data | {
                            'counter': counter_value
                        }
                    )

                    text: str = orjson.dumps(data).decode('utf-8')
                    try:
                        await websocket.send_text(text)
                    except (ConnectionClosedOK, ConnectionClosedError,
                            ClientDisconnected) as exc:
                        _LOGGER.debug(
                            f'WebSocket connection closed: {exc}',
                            extra=log_data
                        )
                        return

    @staticmethod
    @TRACER.start_as_current_span('MemberData.mutate')
    async def mutate(service_id, class_name: str, data: dict[str, object],
                     query_id: UUID, remote_addr: str, auth: RequestAuth,
                     origin_id: UUID | None = None,
                     origin_id_type: IdType | None = None,
                     log_data: dict[str, any] = {}) -> int:
        '''
        Mutates the provided data for a dict

        :param service_id: Service ID for which the Data API was called
        :param class_name: the class for which to change the dict
        :param mutate_data: the data to mutate
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param log_data: additional data to include in log messages
        :returns: number of items affected by the mutation
        :raises: ValueError
        '''

        _LOGGER.debug(
            'Got Rest Data API mutation for object', extra=log_data
        )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)

        if not origin_id:
            origin_id = auth.id
        if not origin_id_type:
            origin_id_type = auth.id_type

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.MUTATE, 'REST Data', class_name,
            query_id=query_id
        )

        return await MemberData.mutate_data(
            server, member, class_name, data, origin_id, origin_id_type,
            log_data=log_data
        )

    async def mutate_data(server: PodServer, member: Member, class_name: str,
                          data: dict[str, object], origin_id: UUID,
                          origin_id_type: IdType, log_data: dict[str, any] = {}
                          ) -> int:

        _LOGGER.debug('Mutating data for data_class', extra=log_data)

        data_store: DataStore = server.data_store

        records_affected: int = await data_store.mutate(
            member.member_id, class_name, data, '', origin_id=origin_id,
            origin_id_type=origin_id_type,
        )

        return records_affected

    @staticmethod
    @TRACER.start_as_current_span('MemberData.append')
    async def append(service_id, class_name: str,
                     query_id: UUID, depth: int, remote_member_id: UUID,
                     data: dict[str, object],
                     append_model: AppendModel,
                     remote_addr: str, auth: RequestAuth,
                     origin_id: UUID | None = None,
                     origin_id_type: IdType | None = None,
                     origin_class_name: str | None = None,
                     log_data: dict[str, any] = {}
                     ) -> int:
        '''
        Appends the provided data


        :param service_id: Service ID for which the Data API was called
        :param class_name: the name of the data class to which to append
        :param query_id: the query id of the incoming request
        :param depth: level of requested recursion for the request
        :param remote_member_id: the member id to proxy the request to
        :param data: the data of the request
        :param append_model: the incoming query, including signatures, that
        can be proxied to another pod
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param origin_class_name: the name of the class from which the data
        was sourced, used for cache-only data classes
        :param log_data: additional data for logging messages
        :returns: the number of objects appended
        :raises: ValueError
        '''

        _LOGGER.debug(
            'Got REST Data API call to append', extra=log_data
        )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)

        remote_member_id: UUID | None = append_model.remote_member_id
        depth: int = append_model.depth

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.APPEND, 'REST Data', class_name,
            query_id=query_id, depth=depth, remote_member_id=remote_member_id
        )

        if remote_member_id and remote_member_id != member.member_id:
            _LOGGER.debug(
                'Received remote-append request', extra=log_data
            )
            if depth != 1:
                raise ValueError(
                    'Must specify depth of 1 for appending to another '
                    'member'
                )

            proxy: DataProxy = DataProxy(member)

            object_count: int = await proxy.proxy_append_request(
                class_name, append_model
            )
            return object_count

        if depth != 0:
            raise ValueError('Must specify depth=0 for appending locally')

        result: int = await MemberData.append_data(
            server, member, class_name, data, auth.id, auth.id_type,
            origin_class_name, log_data=log_data
        )

        return result

    @staticmethod
    async def append_data(server: PodServer, member: Member, class_name: str,
                          data: dict[str, object], origin_id: UUID,
                          origin_id_type: IdType, origin_class_name: str,
                          log_data: dict[str, any] = {}
                          ) -> int:
        '''
        Appends the data to the table for the class, updates counters,
        calls PubSub

        :param server: our server object
        :param member: our membership of the service
        :param class_name: the name of the data class to which to append
        :param data: the data of the request
        :param origin_id: the id of the member, service, or app from which the
        data was sourced, used for cache-only data classes
        :param origin_id_type: the ID type from which the data originates. If
        not specified, auth.id_type will be used
        :param origin_class_name: the name of the class from which the data was
        sourced, used for cache-only data classes
        :param log_data: additional data for logging messages
        :returns: the number of objects appended
        :raises: ValueError
        '''

        schema: Schema = member.schema
        member_id: UUID = member.member_id
        data_class: SchemaDataArray = schema.data_classes[class_name]

        if origin_class_name and not data_class.cache_only:
            raise ByodaValueError(
                'origin_class_name can only be specified for cache-only '
                'classes'
            )

        _LOGGER.debug('Appending data', extra=log_data)

        required_field_names: list[str] = \
            await MemberData.get_required_field_names(
                member.service_id, class_name
            )

        cursor: str = Table.get_cursor_hash(
            data, member_id, required_field_names
        )

        if data_class.cache_only:
            _LOGGER.debug(
                'Using cache for read-only class', extra=log_data
            )
            cache_store: CacheStore = server.cache_store
            object_count: int = await cache_store.append(
                member_id, data_class, data, cursor=cursor,
                origin_id=origin_id, origin_id_type=origin_id_type,
                origin_class_name=origin_class_name
            )
            table: Table = cache_store.get_table(member_id, class_name)
        else:
            data_store: DataStore = server.data_store
            object_count = await data_store.append(
                member_id, data_class, data, cursor=cursor,
                origin_id=origin_id, origin_id_type=origin_id_type,
            )
            table: Table = data_store.get_table(member_id, class_name)

        if config.debug and config.disable_pubsub:
            _LOGGER.debug(
                'Not performing pubsub updates for test cases',
                extra=log_data
            )
            return object_count

        # Update the counter for the top-level array
        counter_cache: CounterCache = member.counter_cache

        if data_class.referenced_class:
            keys: set[str] = MemberData._get_counter_key_permutations(
                data_class, data
            )
        else:
            keys: set[str] = set([data_class.name])

        for key in keys:
            await counter_cache.update(key, 1, table, None)

        message: PubSubDataAppendMessage = PubSubDataAppendMessage.create(
            data, data_class, origin_id, origin_id_type, origin_class_name,
            cursor
        )
        pubsub_class: PubSub = data_class.pubsub_class
        # pubsub_class is None if this function was called by something other
        # than the byoda app server
        if pubsub_class:
            await pubsub_class.send(message)
        else:
            _LOGGER.debug(
                'Not sending PubSubAppend message as there is no pubsub '
                'instance', extra=log_data
            )

        return object_count

    @staticmethod
    @TRACER.start_as_current_span('MemberData.update')
    async def update(service_id, class_name: str,
                     query_id: UUID, data_filter: DataFilterType,
                     depth: int, remote_member_id: UUID,
                     data: dict[str, object],
                     remote_addr: str, auth: RequestAuth,
                     origin_id: UUID | None = None,
                     origin_id_type: IdType | None = None,
                     origin_class_name: str | None = None,
                     log_data: dict[str, any] = {}) -> int:
        '''
        Updates the data matching the filter

        :param service_id: Service ID for which the Data API was called
        :param class_name: the name of the data class to which to append
        :param query_id: a unique ID for the query
        :param data_filter: data filter used to select the record to update
        :param depth: depth of recursion, Proxied updates are not yet supported
        so this value is ignored
        :remote_member_id: the remote_member_id to proxy this request to.
        As proxied updates are not supported this value is ignored
        :param remote_addr: host that originated the Data API query
        :param auth: provides information on the authentication for the request
        :param origin_id: The ID from which the data originates. If not
        specified, auth.id will be used
        :param origin_id_type: the ID type from which the data originates. If
        not specified, auth.id_type will be used
        :param origin_class_name: the name of the class from which the data
        was sourced, used for cache-only data classes
        :param log_data: additional data for logging messages
        :returns: the number of objects updated
        :raises: ValueError
        '''

        if not data_filter:
            raise ValueError(
                'Must specify one or more filters to select content for '
                'update'
            )

        if remote_member_id or depth:
            raise ValueError('Remote updates are not supported yet')

        _LOGGER.debug(
            'Got REST Data API call to update', extra=log_data
        )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)

        schema: Schema = member.schema
        data_class: SchemaDataArray = schema.data_classes[class_name]

        data_filter_set: DataFilterSet = DataFilterSet(data_filter)

        if not origin_id:
            origin_id = auth.id
        if not origin_id_type:
            origin_id_type = auth.id_type

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.UPDATE, 'REST Data', class_name,
            query_id=query_id, depth=depth, remote_member_id=remote_member_id,
            filters=data_filter_set
        )

        return await MemberData.update_data(
            server, member, data_class, data, data_filter_set,
            origin_id, origin_id_type, origin_class_name, log_data
        )

    @staticmethod
    async def update_data(server: PodServer, member: Member,
                          data_class: SchemaDataArray, data: dict[str, object],
                          data_filter_set: DataFilterSet,
                          origin_id: UUID, origin_id_type: IdType,
                          origin_class_name, log_data: dict[str, any] = {}
                          ) -> int:
        '''
        Performs the actual update to the data

        :param server:
        :param member: our membership
        :param class_name: name of the class for which to update data
        :param data_filter: the filter to select the data to update
        :param origin_id: The ID from which the data originates.
        :param origin_id_type: the ID type from which the data originates.
        :param origin_class_name: the name of the class from which the data
        was sourced, used for cache-only data classes
        :param log_data: additional info to add to log messages
        :returns: the number of objects updated
        :raises: ValueError
        '''

        member_id: UUID = member.member_id
        service_id: int = member.service_id

        _LOGGER.debug(
            'Received update request with no remote member ID',
            extra=log_data
        )

        required_field_names: list[str] = \
            await MemberData.get_required_field_names(
                service_id, data_class.name
            )

        cursor: str = Table.get_cursor_hash(
            data, member_id, required_field_names
        )

        update_data: dict[str, object] = {
            key: value for key, value in data.items()
            if value is not None
        }

        if data_class.cache_only:
            _LOGGER.debug(
                'Using cache for read-only class', extra=log_data
            )
            cache_store: CacheStore = server.cache_store
            object_count: int = await cache_store.mutate(
                member_id, data_class.name, update_data, cursor,
                origin_id, origin_id_type, origin_class_name, data_filter_set
            )
        else:
            data_store: DataStore = server.data_store
            object_count = await data_store.mutate(
                member_id, data_class.name, update_data, cursor,
                origin_id, origin_id_type, data_filter_set
            )

        log_data['objects_updated'] = object_count
        _LOGGER.debug(
            'Saving fields of data after mutation',
            extra=log_data | {'fields': len(update_data or [])}
        )

        if object_count == 0:
            # No PubSub message needs to be sent
            return object_count

        message: PubSubDataMutateMessage = PubSubDataMutateMessage.create(
            update_data, data_class, data_filter_set, origin_id=origin_id,
            origin_id_type=origin_id_type,
            origin_class_name=origin_class_name,
            cursor=cursor
        )
        pubsub_class: PubSub = data_class.pubsub_class
        # pubsub_class is None if this function was called by something other
        # than the byoda app server
        if pubsub_class:
            await pubsub_class.send(message)

        return object_count

    @staticmethod
    async def delete(service_id: int, class_name: str,
                     data_filter: DataFilterType, query_id: UUID,
                     remote_member_id: UUID, depth: int, remote_addr: str,
                     auth: RequestAuth, log_data: dict[str, any] = {}) -> int:
        '''
        Deletes one or more objects from an array.

        This function is called from the REST Data API code generated by the
        jsonschema-to-Python converter

        :param service_id: the service being queried
        :param class_name: the name of the data class from which to delete
        :param data_filter: filter to select data to delete
        :param query_id: the query id of the incoming request
        :param remote_member_id: the ID of the member to proxy the request to
        :param depth: the number of hops to proxy this request
        :param remote_addr: host that originated the Data query
        :param auth: provides information on the authentication for the request
        :param filters: filters to select the data to be deleted
        :param log_data: additional data to include in log messages
        :returns: number of objects deleted
        '''

        _LOGGER.debug('Got Rest Data Delete API invocation', extra=log_data)

        if not data_filter:
            raise ValueError(
                'Must specify one or more filter conditions to select content '
                'for deletion'
            )

        if depth != 0 or remote_member_id:
            raise ValueError(
                'Deleting data on remote pods is not currently supported'
            )

        server: PodServer = config.server
        account: Account = server.account
        member: Member = await account.get_membership(service_id)
        schema: Schema = member.schema
        data_class: SchemaDataArray = schema.data_classes[class_name]
        data_filter_set: DataFilterSet = DataFilterSet(
            data_filter, data_class=data_class
        )

        await member.data.add_log_entry(
            remote_addr, auth, DataRequestType.DELETE, 'REST Data', class_name,
            query_id=query_id, depth=depth, filters=data_filter_set,
            remote_member_id=remote_member_id
        )

        result: int = await MemberData.delete_data(
            server, member, data_class, data_filter_set, log_data=log_data
        )
        return result

    @staticmethod
    async def delete_data(server: PodServer, member: Member,
                          data_class: SchemaDataItem,
                          data_filter: DataFilterSet,
                          log_data: dict[str, any] = {}) -> int:
        '''
        Deletes the data and updates counters.

        Input validation is not performed by this method.

        :param server:
        :param member: our membership
        :param data_class: the data class for which to delete data
        :param data_filter: the filter to select the data to be deleted
        :param log_data: additional data for logging messages
        :returns: number of objects deleted
        '''

        _LOGGER.debug('Deleting data for data_class', extra=log_data)

        member_id: UUID = member.member_id

        if data_class.cache_only:
            _LOGGER.debug(
                'Using cache for read-only class', extra=log_data
            )
            cache_store: CacheStore = server.cache_store
            object_count: int = await cache_store.delete(
                member.member_id, data_class.name, data_filter
            )
            table: Table = cache_store.get_table(member_id, data_class.name)
        else:
            data_store: DataStore = server.data_store
            object_count = await data_store.delete(
                member_id, data_class.name, data_filter
            )
            table: Table = data_store.get_table(member_id, data_class.name)

        if config.debug and config.disable_pubsub:
            _LOGGER.debug(
                'Not performing pubsub updates for test-cases', extra=log_data
            )
            return object_count

        # Update the counter for the top-level array
        counter_cache: CounterCache = member.counter_cache

        referenced_class: SchemaDataObject = data_class.referenced_class
        if not referenced_class:
            # Edge case for when the top-level array stores scalars instead
            # of objects
            return object_count

        # We need to see if any of the filters are for fields that are
        # counters and update the counters for those fields. This means that
        # field-specific counters are only decremented if the delete Data API
        # command specified the counter field in the filter.
        # HACK: this means that counters will not be properly decremented if
        # a filter was not specified for a counter field. Because of this
        # reason, the pod_worker will have to periodically check value for
        # the field-specific counters
        filter_data: dict = {}
        for field in referenced_class.fields.values():
            if not field.is_counter:
                continue

            counter_data_filter: str | None = getattr(filter, field.name, None)
            filter_value: str | None = getattr(
                counter_data_filter, 'eq', None
            )
            if counter_data_filter and filter_value:
                _LOGGER.debug(
                    'Filtering on counter field', extra=log_data | {
                        'field': field.name, 'value': filter_value
                    }
                )
                filter_data[field.name] = filter_value

        if object_count == 0:
            # No PubSub message to send as there are no changes
            return object_count

        await MemberData._update_field_counters(
                -1 * object_count, filter_data, data_class,
                counter_cache, table
        )

        message: PubSubDataDeleteMessage = PubSubDataDeleteMessage.create(
            data_class, data_filter
        )
        pubsub_class: PubSub = data_class.pubsub_class
        # pubsub_class is None if this function was called by something other
        # than the byoda app server
        if pubsub_class:
            await pubsub_class.send(message)

        return object_count

    @staticmethod
    async def _update_field_counters(delta: int, data: dict,
                                     data_class: SchemaDataArray,
                                     counter_cache: CounterCache, table: Table
                                     ):
        '''
        Update the counter cache for any fields in the SchemaDataArray that
        are counters

        :param delta: The amount to increment the counter by (can be negative)
        :param data: the data provided in the query. Only counters for fields
        for which data is provided can be updated.
        :param data_class: The data class for the array for which the counters
        should be updated
        :param counter_cache: the cache where the key/values are stored
        :param table: the (SQL) table that can be queried if there is no
        existing value in the cache to start with
        '''

        # TODO: create test cases for this code
        keys: set[str] = MemberData._get_counter_key_permutations(
            data_class, data
        )
        log_data: dict[str, any] = {
            'data_class': data_class.name,
            'delta': delta,
            'keys': len(keys),
        }
        for key in keys:
            log_data['key'] = key
            _LOGGER.debug('Updating counter for append', extra=log_data)
            await counter_cache.update(key, delta, table)

    @staticmethod
    def _get_counter_key_permutations(data_class: SchemaDataArray, data: set
                                      ) -> set[str]:
        '''
        Gets the different key permutations for the fields with the is_counter
        property set to True and for which a key/value exists in the data

        :param data_class: The data class for the array for which the keys
        should be generated
        :param data: the data provided in the query. Only counters for fields
        that have a value in the data will be included in the keys
        '''

        referenced_class: SchemaDataObject = data_class.referenced_class
        counter_fields = []
        if referenced_class:
            counter_fields: list[str] = [
                field.name for field in referenced_class.fields.values()
                if field.is_counter and (not data or data.get(field.name))
            ]

        subsets = set()
        for index in range(1, len(counter_fields) + 1):
            sets = iter_combinations(counter_fields, index)
            for key in sets:
                subsets.add(key)

        keys: set[str] = set()
        for combo in subsets:
            value: str = data_class.name
            for field in combo:
                # We can only manage keys for fields that are counters
                # if a value is provided for the field in the data.
                # This means counters do not have to be accurate but
                # we accept that to avoid having to query the database
                if field in data:
                    value += f'_{field}={data[field]}'

            keys.add(value.rstrip('-'))

        # We always have a key for the array
        keys.add(data_class.name)

        return keys
